{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37e15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5d2175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 21:32:11 WARN Utils: Your hostname, vaibhavi-HP-Laptop-15-fd0xxx resolves to a loopback address: 127.0.1.1; using 192.168.0.128 instead (on interface wlo1)\n",
      "25/07/21 21:32:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 21:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ContentRecommendationEDA\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca92c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/vaibhavi/spark-ml-venv/ml_project/preprocessing/output/content/*\")\n",
    "#/home/vaibhavi/spark-ml-venv/ml_project/preprocessing/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0baa1140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6c5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+------------+\n",
      "|Title|authors|categories|review_count|\n",
      "+-----+-------+----------+------------+\n",
      "|    0|  31103|     40011|           0|\n",
      "+-----+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the null values in th df\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87de384e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188980"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Title\").distinct().count()\n",
    "#df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b84504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4257a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 188980 distinct itles, and total 188980 titles, so it is not needed to get them one hot encoded or numbered as they are different for each entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78920c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112894\n",
      "10409\n"
     ]
    }
   ],
   "source": [
    "print(df.select(\"authors\").distinct().count())\n",
    "\n",
    "print(df.select(\"categories\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc244f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+\n",
      "|               Title|             authors|          categories|review_count|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "|\"\"\"Carefree\"\" (R....|['Allan Scott', '...|                NULL|           1|\n",
      "|\"\"\"Glory is a-com...|['Martha Peterson...|         ['Indiana']|           2|\n",
      "|\"\"\"I Do\"\"...Weddi...|                NULL|['Business & Econ...|          12|\n",
      "|\"\"\"I just got a j...|                NULL|                NULL|          17|\n",
      "|\"\"\"Little Rainman...|['Karen L. Simmons']|['Autistic childr...|           9|\n",
      "|\"\"\"Nothing but pr...|                NULL|         ['History']|           1|\n",
      "|\"\"\"Purse\"\"onalize...|['Andrews McMeel ...|                NULL|           1|\n",
      "|\"\"\"What shall we ...|   ['Clarence Cook']|                NULL|           1|\n",
      "|\"Confessions of a...|['Ed Roth', 'Howi...|['Biography & Aut...|           2|\n",
      "|\"Discovery of the...|['Robert D. Balla...|       ['Derelicts']|           6|\n",
      "|\"Dostoevsky's \"\"T...|      ['Liza Knapp']|['Literary Critic...|           1|\n",
      "|\"El reino de los ...|['New England Mod...|                NULL|           1|\n",
      "|\"Free African Ame...|    ['Paul Heinegg']|['African America...|           7|\n",
      "|\"Geodesic \"\"Airol...|   ['Platt Monfort']|                NULL|           1|\n",
      "|\"Great Sales Peop...|['Joe Miller', 'P...|['Business & Econ...|           4|\n",
      "|\"History of the e...|['John Carroll Po...|       ['Colonists']|           1|\n",
      "|\"How to Build the...|   ['David Stimson']|['Crafts & Hobbies']|           1|\n",
      "|\"Idiot: Beating \"...|['Johnny Damon', ...|['Sports & Recrea...|          45|\n",
      "|\"In My Father's A...|['Dr. Patricia Lo...|      ['Psychology']|          12|\n",
      "|\"John H. Tobe's H...|    ['John H. Tobe']|['Cooking, Canadi...|           1|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb345513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that means there are repeated authors and categories. this results in sparse matrices with traditional trechniques of one hot encoding or string indexer.\n",
    "# so we will use word2vec for generating the final vector embedding for the cosine  similarity!\n",
    "from pyspark.sql.functions import when , col\n",
    "\n",
    "df_final = df \\\n",
    "    .withColumn(\"authors\", when(col(\"authors\").isNull(), \"Unknown\").otherwise(col(\"authors\"))) \\\n",
    "    .withColumn(\"categories\", when(col(\"categories\").isNull(), \"Unknown\").otherwise(col(\"categories\"))) \\\n",
    "    .withColumn(\"review_count\", when(col(\"review_count\").isNull(), 0).otherwise(col(\"review_count\"))) \\\n",
    "    .filter(col(\"Title\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fa6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+\n",
      "|               Title|             authors|          categories|review_count|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "|\"\"\"Carefree\"\" (R....|['Allan Scott', '...|             Unknown|           1|\n",
      "|\"\"\"Glory is a-com...|['Martha Peterson...|         ['Indiana']|           2|\n",
      "|\"\"\"I Do\"\"...Weddi...|             Unknown|['Business & Econ...|          12|\n",
      "|\"\"\"I just got a j...|             Unknown|             Unknown|          17|\n",
      "|\"\"\"Little Rainman...|['Karen L. Simmons']|['Autistic childr...|           9|\n",
      "|\"\"\"Nothing but pr...|             Unknown|         ['History']|           1|\n",
      "|\"\"\"Purse\"\"onalize...|['Andrews McMeel ...|             Unknown|           1|\n",
      "|\"\"\"What shall we ...|   ['Clarence Cook']|             Unknown|           1|\n",
      "|\"Confessions of a...|['Ed Roth', 'Howi...|['Biography & Aut...|           2|\n",
      "|\"Discovery of the...|['Robert D. Balla...|       ['Derelicts']|           6|\n",
      "|\"Dostoevsky's \"\"T...|      ['Liza Knapp']|['Literary Critic...|           1|\n",
      "|\"El reino de los ...|['New England Mod...|             Unknown|           1|\n",
      "|\"Free African Ame...|    ['Paul Heinegg']|['African America...|           7|\n",
      "|\"Geodesic \"\"Airol...|   ['Platt Monfort']|             Unknown|           1|\n",
      "|\"Great Sales Peop...|['Joe Miller', 'P...|['Business & Econ...|           4|\n",
      "|\"History of the e...|['John Carroll Po...|       ['Colonists']|           1|\n",
      "|\"How to Build the...|   ['David Stimson']|['Crafts & Hobbies']|           1|\n",
      "|\"Idiot: Beating \"...|['Johnny Damon', ...|['Sports & Recrea...|          45|\n",
      "|\"In My Father's A...|['Dr. Patricia Lo...|      ['Psychology']|          12|\n",
      "|\"John H. Tobe's H...|    ['John H. Tobe']|['Cooking, Canadi...|           1|\n",
      "+--------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8008317b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188980"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808bf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the cosine model, we need vectors\n",
    "# title is done using tf-idf vectorizer\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Tokenize title\n",
    "tokenizer = RegexTokenizer(inputCol=\"Title\", outputCol=\"title_tokens\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"title_filtered\")\n",
    "\n",
    "# TF-IDF\n",
    "hashingTF = HashingTF(inputCol=\"title_filtered\", outputCol=\"title_tf\", numFeatures=500)\n",
    "idf = IDF(inputCol=\"title_tf\", outputCol=\"title_tfidf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec for authors and categories\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer\n",
    "\n",
    "# Tokenize authors on comma\n",
    "author_tokenizer = RegexTokenizer(inputCol=\"authors\", outputCol=\"author_tokens\", pattern=\"\\\\s*,\\\\s*\")\n",
    "\n",
    "# Word2Vec\n",
    "author_w2v = Word2Vec(vectorSize=50, minCount=1, inputCol=\"author_tokens\", outputCol=\"author_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6325ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean brackets and split by comma\n",
    "category_tokenizer = RegexTokenizer(inputCol=\"categories\", outputCol=\"category_tokens\", pattern=\"\\\\s*,\\\\s*|\\\\[|\\\\]|'\")\n",
    "category_w2v = Word2Vec(vectorSize=30, minCount=1, inputCol=\"category_tokens\", outputCol=\"category_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "230a5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "review_assembler = VectorAssembler(inputCols=[\"review_count\"], outputCol=\"review_count_vec\")\n",
    "review_scaler = MinMaxScaler(inputCol=\"review_count_vec\", outputCol=\"review_count_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a46544f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d90b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final combining all the features\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"title_tfidf\", \"author_vec\", \"category_vec\", \"review_count_scaled\"],\n",
    "    outputCol=\"final_features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b229be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for the eda stesp!\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, remover, hashingTF, idf,\n",
    "    author_tokenizer, author_w2v,\n",
    "    category_tokenizer, category_w2v,\n",
    "    review_assembler, review_scaler,\n",
    "    final_assembler\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb5bd2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#executing the pipeline - \n",
    "model = pipeline.fit(df_final)\n",
    "df_vectorized = model.transform(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd16d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+\n",
      "|               Title|             authors|          categories|review_count|        title_tokens|      title_filtered|            title_tf|         title_tfidf|       author_tokens|          author_vec|     category_tokens|        category_vec|review_count_vec| review_count_scaled|      final_features|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+\n",
      "|\"\"\"Carefree\"\" (R....|['Allan Scott', '...|             Unknown|           1|[carefree, r, k, ...|[carefree, r, k, ...|(500,[23,41,56,15...|(500,[23,41,56,15...|[['allan scott', ...|[-0.0016314059030...|           [unknown]|[-0.0144161665812...|           [1.0]|               [0.0]|(581,[23,41,56,15...|\n",
      "|\"\"\"Glory is a-com...|['Martha Peterson...|         ['Indiana']|           2|[glory, is, a, co...|[glory, comin, so...|(500,[41,43,96,15...|(500,[41,43,96,15...|[['martha peterso...|[-0.0076650250703...|           [indiana]|[0.00726993661373...|           [2.0]|[4.54091363182272...|(581,[41,43,96,15...|\n",
      "|\"\"\"I Do\"\"...Weddi...|             Unknown|['Business & Econ...|          12|[i, do, weddings,...|[weddings, guide,...|(500,[116,144,173...|(500,[116,144,173...|           [unknown]|[-0.0086496993899...|[business & econo...|[0.00893188361078...|          [12.0]|[4.99500499500499...|(581,[116,144,173...|\n",
      "|\"\"\"I just got a j...|             Unknown|             Unknown|          17|[i, just, got, a,...|[got, job, sales,...|(500,[10,57,229,2...|(500,[10,57,229,2...|           [unknown]|[-0.0086496993899...|           [unknown]|[-0.0144161665812...|          [17.0]|[7.26546181091635...|(581,[10,57,229,2...|\n",
      "|\"\"\"Little Rainman...|['Karen L. Simmons']|['Autistic childr...|           9|[little, rainman,...|[little, rainman,...|(500,[90,187,218,...|(500,[90,187,218,...|[['karen l. simmo...|[-0.0053017721511...| [autistic children]|[-0.0062774419784...|           [9.0]|[3.63273090545817...|(581,[90,187,218,...|\n",
      "|\"\"\"Nothing but pr...|             Unknown|         ['History']|           1|[nothing, but, pr...|   [nothing, praise]|(500,[292,446],[1...|(500,[292,446],[4...|           [unknown]|[-0.0086496993899...|           [history]|[0.03495627641677...|           [1.0]|               [0.0]|(581,[292,446,500...|\n",
      "|\"\"\"Purse\"\"onalize...|['Andrews McMeel ...|             Unknown|           1|[purse, onalized,...|[purse, onalized,...|(500,[289,329,344...|(500,[289,329,344...|[['andrews mcmeel...|[-0.0058646453544...|           [unknown]|[-0.0144161665812...|           [1.0]|               [0.0]|(581,[289,329,344...|\n",
      "|\"\"\"What shall we ...|   ['Clarence Cook']|             Unknown|           1|[what, shall, we,...|      [shall, walls]|(500,[157,462],[1...|(500,[157,462],[3...| [['clarence cook']]|[-0.0072320401668...|           [unknown]|[-0.0144161665812...|           [1.0]|               [0.0]|(581,[157,462,500...|\n",
      "|\"Confessions of a...|['Ed Roth', 'Howi...|['Biography & Aut...|           2|[confessions, of,...|[confessions, rat...|(500,[1,48,111,27...|(500,[1,48,111,27...|[['ed roth', 'how...|[-0.0010103594977...|[biography & auto...|[0.01542508974671...|           [2.0]|[4.54091363182272...|(581,[1,48,111,27...|\n",
      "|\"Discovery of the...|['Robert D. Balla...|       ['Derelicts']|           6|[discovery, of, t...|[discovery, titanic]|(500,[228,270],[1...|(500,[228,270],[4...|[['robert d. ball...|[5.79732004553079...|         [derelicts]|[-0.0113747613504...|           [6.0]|[2.27045681591136...|(581,[228,270,500...|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectorized.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6bbb345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vectorized.select(\"Title\", \"final_features\").write.mode(\"overwrite\").parquet(\"vectorized_books.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c646d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188980"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vectorized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7caf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8df2f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- title_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_tf: vector (nullable = true)\n",
      " |-- title_tfidf: vector (nullable = true)\n",
      " |-- author_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- author_vec: vector (nullable = true)\n",
      " |-- category_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- category_vec: vector (nullable = true)\n",
      " |-- review_count_vec: vector (nullable = true)\n",
      " |-- review_count_scaled: vector (nullable = true)\n",
      " |-- final_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectorized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffa863a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|title                                |\n",
      "+-------------------------------------+\n",
      "|Atlas Shrugged                       |\n",
      "|The Hobbit                           |\n",
      "|The Great Gatsby                     |\n",
      "|Brave New World                      |\n",
      "|Of Mice and Men                      |\n",
      "|The Giver                            |\n",
      "|The Picture of Dorian Gray           |\n",
      "|Persuasion                           |\n",
      "|Great Expectations                   |\n",
      "|Pride and Prejudice                  |\n",
      "|Mere Christianity                    |\n",
      "|Wuthering Heights                    |\n",
      "|Harry Potter and The Sorcerer's Stone|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_vectorized.filter(col(\"review_count\") > 5000).select(\"title\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "421814dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55ea2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_vector = df_vectorized.filter(col(\"Title\") == \"The Hobbit\").select(\"final_features\").first()[\"final_features\"]\n",
    "broadcast_vec = spark.sparkContext.broadcast(book_vector.toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c93824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec):\n",
    "    if vec is None:\n",
    "        return 0.0\n",
    "    vec1 = vec.toArray()\n",
    "    vec2 = broadcast_vec.value\n",
    "    dot = float(np.dot(vec1, vec2))\n",
    "    norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return float(dot / norm) if norm != 0 else 0.0\n",
    "\n",
    "cosine_udf = udf(cosine_sim, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c77a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------------+\n",
      "|title                                                |similarity        |\n",
      "+-----------------------------------------------------+------------------+\n",
      "|The Hobbit                                           |1.0000000000000002|\n",
      "|Crucible, The                                        |0.9789467997140651|\n",
      "|Caldecott                                            |0.9783885673884307|\n",
      "|Q                                                    |0.9783778589111998|\n",
      "|The Vision                                           |0.9783415580575551|\n",
      "|Sarkhan                                              |0.9783331754005907|\n",
      "|Remote                                               |0.9783030761963375|\n",
      "|Musclebound                                          |0.9783009609842661|\n",
      "|The Reaches                                          |0.9782770157834391|\n",
      "|When Marian Sang: The True Recital of Marian Anderson|0.8793333615384179|\n",
      "+-----------------------------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sim = df_vectorized.withColumn(\"similarity\", cosine_udf(\"final_features\"))\n",
    "df_sim.orderBy(col(\"similarity\").desc()).select(\"title\", \"similarity\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c51677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
