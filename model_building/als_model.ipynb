{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60a2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a38397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 18:16:24 WARN Utils: Your hostname, vaibhavi-HP-Laptop-15-fd0xxx resolves to a loopback address: 127.0.1.1; using 192.168.0.128 instead (on interface wlo1)\n",
      "25/07/22 18:16:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/22 18:16:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/22 18:16:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/22 18:16:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AlsModel\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caa6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/home/vaibhavi/spark-ml-venv/ml_project/preprocessing/output/als/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abab983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(User_id='A01254073JW8SSTKH6AIB', Id='0451521196', rating=5.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770be20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2380346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3dd862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac34e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|User_id| Id|rating|\n",
      "+-------+---+------+\n",
      "|      0|  0|     0|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the null values in th df\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de99937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# as we can see there are no null values, but the als model needs only numeric data for the clculation, so converting all the user-id and id into numeric:\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# String index userId and itemId (which may both be strings)\n",
    "user_indexer = StringIndexer(inputCol=\"User_id\", outputCol=\"userIdx\", handleInvalid=\"skip\")\n",
    "\n",
    "\n",
    "# Fit and transform\n",
    "df_indexed = user_indexer.fit(df).transform(df)\n",
    "\n",
    "# Cast to int\n",
    "df_ready = df_indexed.select(\n",
    "    col(\"userIdx\").cast(\"int\").alias(\"user\"),\n",
    "    col(\"Id\").cast(\"int\").alias(\"id\"),\n",
    "    col(\"rating\").cast(\"float\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897ac5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:05:39 WARN DAGScheduler: Broadcasting large task binary with size 38.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(user=9323, id=451521196, rating=5.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20790b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:05:45 WARN DAGScheduler: Broadcasting large task binary with size 33.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2380346"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ready.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09b1c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4g'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().get(\"spark.executor.memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d70944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ready.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:12:12 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:12:20 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:12:24 WARN DAGScheduler: Broadcasting large task binary with size 33.4 MiB\n",
      "25/07/21 15:12:30 WARN DAGScheduler: Broadcasting large task binary with size 33.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#checking this to see the manageability of the data : \n",
    "print(df_ready.select(\"user\").distinct().count())\n",
    "print(df_ready.select(\"id\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f45050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:19:03 WARN DAGScheduler: Broadcasting large task binary with size 38.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1345319"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ready.filter(\"user IS NULL OR id IS NULL OR rating IS NULL\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb178de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efefeaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:25:01 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "[Stage 34:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------+\n",
      "|  total|user_nulls|book_nulls|rating_nulls|\n",
      "+-------+----------+----------+------------+\n",
      "|2380346|         0|   1345319|           0|\n",
      "+-------+----------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_ready.selectExpr(\n",
    "    \"count(*) as total\",\n",
    "    \"sum(CASE WHEN user IS NULL THEN 1 ELSE 0 END) as user_nulls\",\n",
    "    \"sum(CASE WHEN id IS NULL THEN 1 ELSE 0 END) as book_nulls\",\n",
    "    \"sum(CASE WHEN rating IS NULL THEN 1 ELSE 0 END) as rating_nulls\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fcb155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ready 's id coulumn has a lot of null values! removing them -- # making the traning and testing sets:\n",
    "\n",
    "df_ready = df_ready.filter(df_ready[\"id\"].isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2994a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:29:03 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "828407"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training_data, test_data) = df_ready.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da95e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:29:40 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:29:42 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:29:48 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:29:55 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:29:59 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:05 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:09 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:12 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:16 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:19 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:23 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:26 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:30 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:33 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:37 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:41 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:45 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:48 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:52 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:55 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:30:58 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:02 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:05 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:08 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:12 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:15 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:19 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:31:22 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# making the als model :\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user\",\n",
    "    itemCol=\"id\",\n",
    "    ratingCol=\"rating\",  # Or `interaction_count` if implicit\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    implicitPrefs=False,\n",
    "    rank=10,\n",
    "    maxIter=10,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "als_model = als.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b63c8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:48:35 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:48:40 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "517890"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.select(\"user\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b194caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get top 1000 users with most reviews\n",
    "top_users_df = df_ready.groupBy(\"user\").count().orderBy(\"count\", ascending=False).limit(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "605eead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Filter ALS recommendations to only these users\n",
    "from pyspark.sql.functions import col\n",
    "user_recs = als_model.recommendForUserSubset(top_users_df, 10)  # top 10 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db58c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:59:47 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:59:55 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:59:58 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 15:59:59 WARN DAGScheduler: Broadcasting large task binary with size 38.3 MiB\n",
      "25/07/21 16:00:35 WARN DAGScheduler: Broadcasting large task binary with size 38.3 MiB\n",
      "[Stage 260:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user|recommendations                                                                                                                                                                                                                                  |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1   |[{738530204, 150.38947}, {1592862578, 7.5992494}, {1884778151, 7.4905157}, {373122241, 7.4680004}, {1861761767, 7.4159055}, {1411610628, 7.403044}, {252023285, 7.368531}, {820701785, 7.3497825}, {873416376, 7.3117695}, {664223591, 7.310601}]|\n",
      "|3   |[{738530204, 83.34863}, {395930979, 6.2096066}, {965611884, 6.1506443}, {1884778151, 6.084033}, {880015454, 6.0798883}, {933873042, 6.0303597}, {848727096, 6.0015345}, {199256047, 5.98658}, {802788815, 5.945369}, {802788807, 5.945369}]      |\n",
      "|5   |[{738530204, 137.84962}, {439401569, 6.587307}, {1411610628, 6.4452133}, {1560975342, 6.4252877}, {345379128, 6.4013524}, {525248951, 6.3825216}, {1861082010, 6.3692055}, {924022043, 6.356049}, {963660713, 6.354623}, {1892805022, 6.3543377}]|\n",
      "|6   |[{738530204, 147.90314}, {192891626, 8.264016}, {760306613, 8.252088}, {821222260, 8.183211}, {851157017, 8.155926}, {929355547, 8.114758}, {785262911, 8.026791}, {295973722, 8.018218}, {1411610628, 7.9977794}, {764545752, 7.961726}]        |\n",
      "|12  |[{738530204, 155.66467}, {1592862578, 7.222651}, {1411610628, 7.133924}, {439401569, 7.083996}, {1888645180, 7.0416102}, {924022043, 7.0307436}, {801012864, 7.012501}, {873493591, 7.009742}, {806525789, 6.9957356}, {226777138, 6.987987}]    |\n",
      "|13  |[{738530204, 85.83541}, {1570420319, 8.081421}, {788168746, 8.081421}, {671887289, 8.081421}, {975375601, 8.057082}, {877287368, 7.932313}, {609804103, 7.9176073}, {281040346, 7.863638}, {803610386, 7.8624163}, {395930979, 7.8550167}]       |\n",
      "|15  |[{738530204, 135.46927}, {849902010, 6.8562517}, {1884778151, 6.84459}, {976938502, 6.827189}, {1844250733, 6.7433767}, {689823819, 6.675109}, {1570761108, 6.6743326}, {1858288797, 6.6400666}, {767910001, 6.614778}, {976392909, 6.590057}]   |\n",
      "|16  |[{738530204, 150.17006}, {1411610628, 6.934412}, {300060904, 6.869566}, {252023285, 6.8272653}, {1888645180, 6.759036}, {1573221406, 6.748919}, {964862301, 6.745929}, {192891626, 6.719649}, {1884778151, 6.688876}, {764545752, 6.6808496}]    |\n",
      "|19  |[{738530204, 143.1746}, {965905608, 7.2533793}, {963660713, 7.0941243}, {498018091, 6.9968452}, {967533007, 6.962961}, {764421255, 6.955876}, {1592862578, 6.932498}, {1558066020, 6.905028}, {1576751554, 6.884895}, {844283576, 6.8632445}]    |\n",
      "|20  |[{738530204, 132.57957}, {471303771, 7.5391855}, {1411610628, 7.5302734}, {664223591, 7.4605694}, {262561565, 7.425281}, {20783604, 7.36607}, {1883707641, 7.3653545}, {373122241, 7.3639526}, {1592862578, 7.340724}, {1560975342, 7.3280177}]  |\n",
      "|22  |[{738530204, 102.648636}, {965611884, 6.957338}, {1413761895, 6.8309374}, {880015454, 6.801124}, {345379128, 6.7779417}, {1556523157, 6.738001}, {1571882707, 6.68306}, {934601976, 6.6624107}, {1930556489, 6.619135}, {1892805022, 6.617384}]  |\n",
      "|26  |[{738530204, 161.21593}, {691088349, 8.953777}, {935848916, 8.764376}, {664226884, 8.724482}, {201657651, 8.672571}, {785262911, 8.654815}, {345379128, 8.573572}, {471303771, 8.537097}, {691015643, 8.522537}, {847814157, 8.512112}]          |\n",
      "|27  |[{738530204, 142.75731}, {924022043, 6.366252}, {252023285, 6.2892747}, {300060904, 6.288425}, {964862301, 6.245598}, {1888645180, 6.208738}, {1884778151, 6.205885}, {764545752, 6.1998606}, {1598791249, 6.196563}, {1557989818, 6.1540728}]   |\n",
      "|28  |[{738530204, 134.99838}, {1858288797, 7.2635345}, {849902010, 7.0955}, {924022043, 7.0543833}, {1411610628, 7.02472}, {1884778151, 6.987541}, {446677396, 6.985767}, {976938502, 6.91342}, {801012864, 6.8623757}, {192891626, 6.8469505}]       |\n",
      "|31  |[{738530204, 134.17598}, {395930979, 7.463007}, {848727096, 7.4565525}, {1411610628, 7.43862}, {821222260, 7.4186425}, {1884778151, 7.3934174}, {295973722, 7.335706}, {192891626, 7.3296}, {1858288797, 7.3188267}, {226777138, 7.3061833}]     |\n",
      "|34  |[{738530204, 122.024086}, {1858288797, 8.216412}, {689823819, 8.11705}, {873896610, 8.091815}, {252023285, 8.085858}, {804114161, 8.057142}, {1573221406, 8.015078}, {1570420319, 7.9816885}, {788168746, 7.9816885}, {671887289, 7.9816885}]    |\n",
      "|40  |[{738530204, 114.34535}, {826340784, 6.8312025}, {1413752225, 6.7913756}, {965905608, 6.7883954}, {785248021, 6.7658873}, {847687503, 6.730272}, {380009307, 6.7136793}, {295973722, 6.6843266}, {741414708, 6.6708937}, {373168101, 6.618391}]  |\n",
      "|41  |[{738530204, 204.12062}, {373111150, 8.08331}, {520041550, 8.082981}, {1854967681, 7.967658}, {761451374, 7.9636326}, {375811753, 7.8827343}, {935782249, 7.8819447}, {966934547, 7.8733935}, {936622237, 7.8728023}, {446671932, 7.848897}]     |\n",
      "|43  |[{738530204, 130.40506}, {848727096, 8.03945}, {345379128, 7.940322}, {1884778151, 7.8766694}, {199256047, 7.845074}, {1411610628, 7.8411818}, {252023285, 7.8297076}, {395930979, 7.829345}, {965611884, 7.8159895}, {691088349, 7.786375}]     |\n",
      "|44  |[{738530204, 134.31566}, {595350593, 7.2322125}, {964459035, 7.178193}, {1411610628, 7.1241593}, {1841767840, 7.089282}, {1932310142, 7.0876784}, {525248951, 7.0798025}, {135321026, 7.075366}, {883442019, 7.056804}, {1578630207, 7.009881}]  |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d76ec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = als_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60b73698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 16:05:00 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 16:05:06 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 16:05:09 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 16:05:13 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/21 16:05:20 WARN DAGScheduler: Broadcasting large task binary with size 38.3 MiB\n",
      "25/07/21 16:05:23 WARN DAGScheduler: Broadcasting large task binary with size 38.3 MiB\n",
      "[Stage 342:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.3773746870659802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dc9a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 18:18:47 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/22 18:18:58 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "[Stage 13:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   0| 3889|\n",
      "|   1| 2155|\n",
      "|   2|  611|\n",
      "|   3|  610|\n",
      "|   4|  559|\n",
      "|  21|  508|\n",
      "|  12|  495|\n",
      "|  10|  449|\n",
      "|   5|  436|\n",
      "|   8|  399|\n",
      "|  16|  396|\n",
      "|  17|  389|\n",
      "|  33|  367|\n",
      "|   6|  357|\n",
      "|   9|  332|\n",
      "|  55|  318|\n",
      "|  18|  317|\n",
      "|  23|  309|\n",
      "|  50|  307|\n",
      "|  39|  304|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eae3a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 18:20:06 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/22 18:20:17 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/22 18:20:20 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_users_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06613b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 18:24:00 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/22 18:24:08 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/07/22 18:24:11 WARN DAGScheduler: Broadcasting large task binary with size 38.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_users_df.write.mode(\"overwrite\").parquet(\"output/top_users.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afcf1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81748aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
